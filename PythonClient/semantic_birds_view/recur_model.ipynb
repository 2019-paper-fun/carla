{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "multi_model = load_model('models/multi_model__sweep=15_decimation=2_numclasses=3_valloss=0.057.h5')\n",
    "print(multi_model.inputs)\n",
    "\n",
    "for l in multi_model.layers:\n",
    "    print('{}: {}'.format(l.name, 'trainable' if l.trainable else 'not-trainable'))\n",
    "\n",
    "SVG(model_to_dot(multi_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Add, Subtract, Average, Flatten, Reshape, Dense, Input, Lambda, Concatenate, Softmax, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import ConvLSTM2D, TimeDistributed\n",
    "\n",
    "from utils import (\n",
    "    sequential_batcher, extract_observation_for_batch,\n",
    "    get_X_and_Y, plot_semantic\n",
    ")\n",
    "\n",
    "\n",
    "def get_recur_model(multi_model, seq_len, input_names, train_encoder=True, train_decoder=True):\n",
    "    input_shape =  K.int_shape(multi_model.input[0])[1:]\n",
    "    seq_input_shape = (seq_len, *input_shape)\n",
    "    encoded_dim = K.int_shape(multi_model.get_layer('encoder_submodel').outputs[0])[1:]\n",
    "\n",
    "    inputs = {\n",
    "        inp_name: Input(seq_input_shape, name='input_from_'+inp_name)\n",
    "        for inp_name in input_names\n",
    "    }\n",
    "\n",
    "    decoder = multi_model.get_layer('decoder_submodel')\n",
    "    decoder.trainable = train_decoder\n",
    "    \n",
    "    previous_reconstruction_model = Model(\n",
    "        [multi_model.inputs[inp_idx] for inp_idx, inp_name in enumerate(input_names)],\n",
    "        multi_model.get_layer('before_reconstruction').output,\n",
    "    )\n",
    "\n",
    "    for_reconstruction = []\n",
    "    for inp_idx, inp_name in enumerate(input_names):\n",
    "        encoder = Model(\n",
    "            multi_model.inputs[inp_idx],\n",
    "            multi_model.get_layer('encoded_from_'+inp_name).output,\n",
    "        )\n",
    "        encoder.trainable = train_encoder\n",
    "\n",
    "        inp = inputs[inp_name]\n",
    "        x = TimeDistributed(encoder)(inp)\n",
    "        for_reconstruction.append(x)\n",
    "\n",
    "    x = Concatenate()(for_reconstruction)\n",
    "    rec = ConvLSTM2D(\n",
    "            encoded_dim[-1],\n",
    "            (3, 3),\n",
    "            padding='same',\n",
    "            return_sequences=False,\n",
    "    )(x)\n",
    "    rec = Convolution2D(\n",
    "        encoded_dim[-1],\n",
    "        (3, 3),\n",
    "        activation='elu',\n",
    "        padding='same',\n",
    "    )(rec)\n",
    "    rec = decoder(rec)\n",
    "    \n",
    "    previous_reconstruction = previous_reconstruction_model([\n",
    "        Lambda(\n",
    "            lambda x: x[:, -1, :, :, :],\n",
    "        )(inputs[inp_name])\n",
    "        for inp_name in input_names\n",
    "    ])\n",
    "    previous_reconstruction = decoder(previous_reconstruction)\n",
    "    \n",
    "    added_reconstructions = Add()([rec, previous_reconstruction])\n",
    "    out = Softmax(axis=3)(added_reconstructions)\n",
    "\n",
    "    return Model([inputs[inp_name] for inp_name in input_names], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECIMATION = 2\n",
    "SEQUENCE_LEN = 4\n",
    "SEQ_BATCH_SIZE = 8\n",
    "CLASSES_NAMES = [\n",
    "    ['Roads', 'RoadLines'],\n",
    "    \n",
    "    ['None', 'Buildings', 'Fences', 'Other', 'Pedestrians',\n",
    "     'Poles', 'Walls', 'TrafficSigns',\n",
    "     'Vegetation', 'Sidewalks'],\n",
    "    \n",
    "    ['Vehicles'],\n",
    "]\n",
    "\n",
    "# CLASSES_NAMES = [\n",
    "#     ['Roads', 'RoadLines'],\n",
    "    \n",
    "#     ['Sidewalks'],\n",
    "    \n",
    "#     ['Buildings'],\n",
    "    \n",
    "#     ['Fences', 'Other', 'Pedestrians',\n",
    "#      'Poles', 'Walls', 'TrafficSigns'],\n",
    "    \n",
    "#     ['Vehicles'],\n",
    "    \n",
    "#     ['Vegetation'],\n",
    "        \n",
    "#     ['None']\n",
    "# ]\n",
    "\n",
    "\n",
    "patience = 10\n",
    "\n",
    "\n",
    "input_names = ['FrontSS', 'LeftSS', 'RightSS', 'RearSS']\n",
    "train_encoder = False\n",
    "train_decoder = False\n",
    "\n",
    "\n",
    "recur_model = get_recur_model(\n",
    "    multi_model,\n",
    "    SEQUENCE_LEN,\n",
    "    input_names,\n",
    "    train_encoder,\n",
    "    train_decoder,\n",
    ")\n",
    "recur_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(1e-5),  # needs to be low!!!!\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "X_val, Y_val = get_X_and_Y(['Town01', 'Town02'], [100, 101], DECIMATION)\n",
    "valid_seq_gen = sequential_batcher(\n",
    "    X_val, Y_val,\n",
    "    return_sequences=False,\n",
    "    batch_size=SEQ_BATCH_SIZE,\n",
    "    sequence_len=SEQUENCE_LEN,\n",
    "    classes_names=CLASSES_NAMES,\n",
    "    val_part=1.0,\n",
    "    validation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RECURRENT_EPISODES = [\n",
    "    range(20, 27),\n",
    "    range(27, 34),\n",
    "    range(34, 40),\n",
    "]\n",
    "\n",
    "\n",
    "for sweep in range(12):\n",
    "    histories = []\n",
    "    for episodes in RECURRENT_EPISODES:\n",
    "        start_time = time.time()\n",
    "        X, Y = get_X_and_Y(['Town01', 'Town02'], episodes, DECIMATION)\n",
    "        print('Reading data took {:.2f} [s]'.format(time.time() - start_time))\n",
    "\n",
    "        train_seq_gen = sequential_batcher(\n",
    "            X, Y,\n",
    "            return_sequences=False,\n",
    "            batch_size=SEQ_BATCH_SIZE,\n",
    "            sequence_len=SEQUENCE_LEN,\n",
    "            classes_names=CLASSES_NAMES,\n",
    "            val_part=0.0,\n",
    "            validation=False,\n",
    "        )\n",
    "        \n",
    "        one_batch_X, one_batch_Y = next(train_seq_gen)\n",
    "\n",
    "        preds = recur_model.predict(one_batch_X)\n",
    "\n",
    "        sep = np.zeros_like(preds[0:1, :, ::5])\n",
    "        for j in range(len(preds)):\n",
    "            plot_semantic(np.concatenate([one_batch_Y[j:j+1], sep, preds[j:j+1]], axis=2))\n",
    "        \n",
    "        print('\\n#### episodes = {} #### (sweep: {})'.format(episodes, sweep))\n",
    "\n",
    "        history = recur_model.fit_generator(\n",
    "            train_seq_gen,\n",
    "            steps_per_epoch=X[0].shape[-1] // SEQ_BATCH_SIZE // 10,\n",
    "            epochs=50,\n",
    "            validation_data=valid_seq_gen,\n",
    "            validation_steps=X_val[0].shape[-1] // SEQ_BATCH_SIZE // 2,\n",
    "            callbacks=[early_stopping],\n",
    "        )\n",
    "        \n",
    "        histories.append(history.history)\n",
    "\n",
    "        del X\n",
    "        del Y\n",
    "        \n",
    "    val_loss = history.history['val_loss'][-(patience+1)]\n",
    "    model_filename = 'models/recur_model__sweep={}_decimation={}_numclasses={}_valloss={:.3f}.h5'.format(sweep, DECIMATION, len(CLASSES_NAMES), val_loss)\n",
    "    multi_model.save(model_filename)\n",
    "    histories_filename = 'histories/recur_model__sweep={}_decimation={}_numclasses={}_valloss={:.3f}.pkl'.format(sweep, DECIMATION, len(CLASSES_NAMES), val_loss)\n",
    "    with open(histories_filename, 'wb') as output:\n",
    "        pickle.dump(histories, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
